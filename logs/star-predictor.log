INFO:root:pipe=
vect: CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 2), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)
tfidf: TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)
clf: SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=-1,
       penalty='l2', power_t=0.5, random_state=None, shuffle=True,
       verbose=0, warm_start=False)

INFO:root:test accuracy= 0.6096695847
INFO:root:train accuracy= 0.597815772263
INFO:root:classification report=
             precision    recall  f1-score   support

          1       0.60      0.84      0.70     82937
          2       0.57      0.10      0.17     56163
          3       0.57      0.21      0.31     80533
          4       0.51      0.20      0.29    168556
          5       0.61      0.97      0.75    283078

avg / total       0.58      0.60      0.53    671267

INFO:root:pipe=
vect: CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)
tfidf: TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)
clf: MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)

INFO:root:test accuracy= 0.574990850626
INFO:root:train accuracy= 0.562875875024
INFO:root:classification report=
             precision    recall  f1-score   support

          1       0.72      0.64      0.68     82990
          2       0.39      0.01      0.02     56336
          3       0.29      0.01      0.03     80232
          4       0.38      0.42      0.40    168479
          5       0.62      0.89      0.73    283230

avg / total       0.51      0.56      0.50    671267

INFO:root:pipe=
vect: CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)
tfidf: TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)
clf: SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=-1,
       penalty='l2', power_t=0.5, random_state=None, shuffle=True,
       verbose=0, warm_start=False)

INFO:root:test accuracy= 0.602547225418
INFO:root:train accuracy= 0.597218394469
INFO:root:classification report=
             precision    recall  f1-score   support

          1       0.60      0.87      0.71     82612
          2       0.50      0.08      0.14     55922
          3       0.49      0.16      0.24     80567
          4       0.46      0.24      0.32    169069
          5       0.63      0.96      0.76    283097

avg / total       0.56      0.60      0.53    671267

INFO:root:pipe=
vect: CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)
tfidf: TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)
clf: SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=-1,
       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,
       verbose=0, warm_start=False)

INFO:root:test accuracy= 0.588425657178
INFO:root:train accuracy= 0.588159406019
INFO:root:classification report=
             precision    recall  f1-score   support

          1       0.59      0.85      0.70     83129
          2       0.42      0.09      0.15     56165
          3       0.48      0.11      0.18     80340
          4       0.43      0.23      0.30    168348
          5       0.63      0.96      0.76    283285

avg / total       0.54      0.59      0.52    671267

INFO:root:pipe=
vect: CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words='english',
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)
tfidf: TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)
clf: SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=-1,
       penalty='l2', power_t=0.5, random_state=None, shuffle=True,
       verbose=0, warm_start=False)

INFO:root:test accuracy= 0.595609095049
INFO:root:train accuracy= 0.589580599076
INFO:root:classification report=
             precision    recall  f1-score   support

          1       0.59      0.85      0.70     82742
          2       0.44      0.10      0.16     56212
          3       0.49      0.16      0.24     80591
          4       0.48      0.21      0.30    168362
          5       0.62      0.96      0.75    283360

avg / total       0.55      0.59      0.52    671267
INFO:root:pipe=
preprocessor: NLTKPreprocessor(lower=True,
         punct={'{', '*', '%', '_', '<', '}', '/', '@', '?', '~', '!', '&', '$', '[', '>', ']', '`', ';', '+', '^', '\\', '"', ',', '#', ')', '-', ':', '.', '(', "'", '|', '='},
         stopwords={'be', 'the', 'o', 'ours', 'm', 'didn', 'against', 'any', 'me', 'haven', 'shan', 'needn', 'there', 'very', 'below', 'don', 'were', 'of', 'is', 'because', 'hadn', 'once', 'does', 'now', 'hasn', 'yours', 'wouldn', 'this', 'while', 'in', 'mightn', 'to', 'shouldn', 'as', 'what', 'on', 'at', 'd...ow', 'from', 'further', 'own', 'then', 'and', 'theirs', 'herself', 'whom', 'into', 'until', 'doing'},
         strip=True)
vect: CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)
tfidf: TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)
clf: SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=-1,
       penalty='l2', power_t=0.5, random_state=None, shuffle=True,
       verbose=0, warm_start=False)

INFO:root:test accuracy= 0.62018
INFO:root:train accuracy= 0.58978
INFO:root:classification report=
             precision    recall  f1-score   support

          1       0.60      0.87      0.71      7248
          2       0.45      0.11      0.18      4616
          3       0.47      0.15      0.22      6174
          4       0.48      0.30      0.37     12536
          5       0.63      0.93      0.75     19426

avg / total       0.55      0.59      0.53     50000

INFO:root:pipe=
preprocessor: NLTKPreprocessor(lower=True,
         punct={'/', ')', '(', ']', '.', ':', '_', '=', '`', '@', '&', '<', '-', '!', '>', ',', '~', '|', '{', '+', '$', "'", '*', '?', '\\', '"', ';', '}', '^', '%', '[', '#'},
         stopwords={'it', 'him', 'his', 'this', 'further', 'myself', 'i', 'doing', 'weren', 'won', 'how', 'here', 'he', 'whom', 'such', 'ours', 'but', 'themselves', 'needn', 'during', 'theirs', 'mightn', 'who', 'does', 'again', 'very', 'what', 'up', 'now', 'are', 'too', 'is', 'shan', 'at', 'hers', 'no', 'fro...an', 'doesn', 'haven', 'when', 'do', 'wasn', 'had', 'out', 'your', 'these', 'a', 's', 'of', 'their'},
         strip=True)
vect: CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 2), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)
tfidf: TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)
clf: SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=-1,
       penalty='l2', power_t=0.5, random_state=None, shuffle=True,
       verbose=0, warm_start=False)

INFO:root:test accuracy= 0.601133479558
INFO:root:train accuracy= 0.582954323689
INFO:root:classification report=
             precision    recall  f1-score   support

          1       0.60      0.81      0.69     82950
          2       0.55      0.10      0.17     55995
          3       0.56      0.17      0.26     80401
          4       0.49      0.19      0.27    168839
          5       0.59      0.97      0.74    283082

avg / total       0.56      0.58      0.51    671267
INFO:root:pipe= 
vect: CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 3), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)
tfidf: TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)
clf: SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=-1,
       penalty='l2', power_t=0.5, random_state=None, shuffle=True,
       verbose=0, warm_start=False)

INFO:root:test accuracy= 0.59696672806
INFO:root:train accuracy= 0.585699877992
INFO:root:classification report= 
             precision    recall  f1-score   support

          1       0.60      0.78      0.68     82720
          2       0.59      0.10      0.17     56125
          3       0.58      0.21      0.31     80411
          4       0.51      0.19      0.27    168951
          5       0.59      0.97      0.73    283060

avg / total       0.57      0.59      0.51    671267

INFO:root:pipe= 
vect: CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 3), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)
tfidf: TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)
clf: OneVsOneClassifier(estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=-1,
       penalty='l2', power_t=0.5, random_state=None, shuffle=True,
       verbose=0, warm_start=False),
          n_jobs=1)

INFO:root:test accuracy= 0.587374410256
INFO:root:train accuracy= 0.599524481317
INFO:root:classification report= 
             precision    recall  f1-score   support

          1       0.74      0.70      0.72     82927
          2       0.52      0.17      0.26     56023
          3       0.54      0.09      0.16     80584
          4       0.45      0.39      0.42    169179
          5       0.63      0.92      0.75    282554

avg / total       0.58      0.60      0.55    671267

INFO:root:pipe= 
vect: CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 2), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)
tfidf: TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)
clf: OneVsOneClassifier(estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=-1,
       penalty='l2', power_t=0.5, random_state=None, shuffle=True,
       verbose=0, warm_start=False),
          n_jobs=-1)

INFO:root:test accuracy= 0.639469480321
INFO:root:train accuracy= 0.637641951712
INFO:root:classification report= 
             precision    recall  f1-score   support

          1       0.73      0.77      0.75     83250
          2       0.54      0.24      0.33     56531
          3       0.56      0.22      0.32     80667
          4       0.50      0.47      0.49    168232
          5       0.69      0.89      0.78    282587

avg / total       0.62      0.64      0.61    671267

INFO:trend-prediction:SVR(C=1.0, cache_size=5000, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',
  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=True)
INFO:trend-prediction:mean squared error: 0.025024983277 
INFO:trend-prediction:r2 score: 0.932726208154 
INFO:trend-prediction:SVR(C=1.0, cache_size=5000, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',
  kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=True)
INFO:trend-prediction:mean squared error: 0.0261864544104 
INFO:trend-prediction:r2 score: 0.92808082073 
INFO:root:pipe= 
vect: CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 2), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)
tfidf: TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)
clf: OneVsOneClassifier(estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=-1,
       penalty='l2', power_t=0.5, random_state=None, shuffle=True,
       verbose=0, warm_start=False),
          n_jobs=-1)

INFO:root:test accuracy= 0.639424788671
INFO:root:train accuracy= 0.638774139053
INFO:root:classification report= 
             precision    recall  f1-score   support

          1       0.74      0.77      0.75     82826
          2       0.53      0.25      0.34     55926
          3       0.57      0.22      0.32     80661
          4       0.50      0.48      0.49    168658
          5       0.69      0.89      0.78    283196

avg / total       0.62      0.64      0.61    671267

INFO:trend-prediction:SVR(C=1.0, cache_size=5000, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',
  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)
INFO:trend-prediction:mean squared error: 0.0250716424493 
INFO:trend-prediction:r2 score: 0.936524132625 
